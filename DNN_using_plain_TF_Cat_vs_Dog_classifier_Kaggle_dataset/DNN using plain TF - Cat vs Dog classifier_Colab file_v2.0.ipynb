{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/selvam-learn/Cat-Dog-Classifier/blob/master/DNN_using_plain_TF_Cat_vs_Dog_classifier_Kaggle_dataset/DNN%20using%20plain%20TF%20-%20Cat%20vs%20Dog%20classifier_Colab%20file.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "8xkS3MVSpU73"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "import time\n",
    "from random import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.contrib.layers import fully_connected, l2_regularizer\n",
    "from tensorflow.contrib.framework import arg_scope\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    lmd_tic = time.time()\n",
    "    \n",
    "    X_full_dataset = []\n",
    "    Y_full_dataset = []\n",
    "    filename_prefix = 'datasets/dog_vs_cat_normalized_dataset_kaggle_128px_'\n",
    "    \n",
    "    for i in range(1, 26):\n",
    "        \n",
    "        filename = filename_prefix + str(i) + \".h5\"\n",
    "        with h5py.File(filename, \"r\") as f:\n",
    "    \n",
    "            #print(list(f.keys()))\n",
    "            #x_mini_data = None\n",
    "            #y_mini_data = None\n",
    "            \n",
    "            X_full_dataset.append(f[\"input_data\"][:])\n",
    "            Y_full_dataset.append(f[\"input_labels\"][:])\n",
    "\n",
    "            #x_mini_data = f[\"input_data\"][:]\n",
    "            #y_mini_data = f[\"input_labels\"][:]\n",
    "    \n",
    "    lmd_toc = time.time()\n",
    "    print('Time taken to load the data set is', ((lmd_toc-lmd_tic) * 1000), 'ms')\n",
    "    \n",
    "    #return x_mini_data, y_mini_data\n",
    "    return X_full_dataset, Y_full_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shuffled_indices(start_value, last_value):\n",
    "    \n",
    "    temp = np.arange(start_value, last_value)\n",
    "    shuffle(temp)\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_random_mini_batches(X, Y, n_classes = 2, mini_batch_size = 32):\n",
    "    \n",
    "    m = X.shape[0] # number of training examples\n",
    "    mini_batches = []\n",
    "    \n",
    "    #print(\"Shape of X =\", X.shape)\n",
    "    #print(\"Shape of Y =\", Y.shape)\n",
    "    \n",
    "    #Reshaping to convert Y to a 2D array from a rank one array\n",
    "    Y = Y.reshape(Y.shape[0], 1)\n",
    "    \n",
    "    #Shuffle the data in each of the mini batch\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation, :]\n",
    "    shuffled_Y = Y[permutation, :]\n",
    "    \n",
    "    n_mini_batches = math.ceil(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(n_mini_batches):\n",
    "        \n",
    "        start_pos = k * mini_batch_size\n",
    "        end_pos = min(start_pos + mini_batch_size, m)\n",
    "        \n",
    "        mini_batch_X = shuffled_X[start_pos : end_pos, :]\n",
    "        mini_batch_Y = shuffled_Y[start_pos : end_pos, :]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i56i0MpqpU8U"
   },
   "source": [
    "### Build the DNN using plain TensorFlow\n",
    "\n",
    "**Step 1 - Code the individual ops in the computation graph**\n",
    "- Create Placeholders for X, Y\n",
    "- Create the nn_model - all the layers and initialize them\n",
    "- Compute the cost\n",
    "- Create the optimizer to minimize the cost\n",
    "- Evaluate the model\n",
    "\n",
    "**Step 2 - Build the computation graph**\n",
    "- Combine all the steps in Step 1 to build the computation graph\n",
    "- Initialize the variables\n",
    "- Create a Saver object to save the learnt parameters after the model is trained\n",
    "\n",
    "**Step 3 - Execute the graph**\n",
    "- Create mini batches so that gradient descent works on these mini batches for every step instead of all instances\n",
    "- Train the model for a given number of epochs\n",
    "- Print the cost, train accuracy & test accuracy at regular interval of epochs\n",
    "- Plot the training error (cost) vs epochs\n",
    "- Save the parameters\n",
    "\n",
    "**Step 4 - Predict for any data**\n",
    "- Build a method to pass random data for the model to predict based on the parameters learnt through training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oq2WIyiCpU8V"
   },
   "source": [
    "### Create the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AkBHP2R4pU8X"
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_inputs):\n",
    "    '''\n",
    "    n_inputs - A scalar containing the number of input features\n",
    "    '''\n",
    "    \n",
    "    X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
    "    Y = tf.placeholder(tf.float32, shape = (None, 1), name = 'Y')\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HoJ-w6NMpU8a"
   },
   "source": [
    "### Create the NN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "0iig8mn_pU8c"
   },
   "outputs": [],
   "source": [
    "def nn_model(X, n_neurons):\n",
    "    '''\n",
    "    X - input Tensor X\n",
    "    n_neurons - A scalar containing the number of neurons in each layer including both hidden layers and output layer \n",
    "    '''\n",
    "    #with arg_scope([fully_connected], weights_regularizer = l2_regularizer(scale = 0.01)):\n",
    "    hidden_layer_1 = fully_connected(X, n_neurons['hidden_layer_1'], scope = 'hidden_layer_1')\n",
    "    hidden_layer_2 = fully_connected(hidden_layer_1, n_neurons['hidden_layer_2'], scope = 'hidden_layer_2')\n",
    "    hidden_layer_3 = fully_connected(hidden_layer_2, n_neurons['hidden_layer_3'], scope = 'hidden_layer_3')\n",
    "    hidden_layer_4 = fully_connected(hidden_layer_3, n_neurons['hidden_layer_4'], scope = 'hidden_layer_4')\n",
    "    hidden_layer_5 = fully_connected(hidden_layer_4, n_neurons['hidden_layer_5'], scope = 'hidden_layer_5')\n",
    "    logits = fully_connected(hidden_layer_3, n_neurons['output_layer'], activation_fn = None, scope = 'output_layer')\n",
    "    \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZMtdnd7pU8f"
   },
   "source": [
    "### Compute the cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iwvRAcarpU8g"
   },
   "outputs": [],
   "source": [
    "def compute_cost(labels, logits):\n",
    "    '''\n",
    "    labels - label tensor Y\n",
    "    logits - Tensor containing the values of the output layer before passing to the activation function\n",
    "    '''\n",
    "    \n",
    "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "    cost = tf.reduce_mean(cross_entropy, name = 'cost')\n",
    "    #base_cost = tf.reduce_mean(cross_entropy)\n",
    "    #reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    #cost = tf.add_n([base_cost] + reg_losses, name = 'cost')\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AAWyNQI6pU8m"
   },
   "source": [
    "### Create the optimizer and the training operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "xk2mXeZ6pU8o"
   },
   "outputs": [],
   "source": [
    "def optimizer(learning_rate, cost):\n",
    "    '''\n",
    "    learning_rate - A scalar value containing the learning rate for the backpropagation step\n",
    "    cost - Overall cost from the forward propagation step for one set of mini batch instance\n",
    "    '''\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(cost)\n",
    "    \n",
    "    return training_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rnft-gRwpU8r"
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "We will use accuracy as the evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "kaxqAWcmpU8r"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(logits, Y):\n",
    "    '''\n",
    "    logits - An array containing the values from the output layer\n",
    "    Y - An array containing the labels\n",
    "    '''\n",
    "    \n",
    "    #Compute the probability using the sigmoid function\n",
    "    y_pred = tf.nn.sigmoid(logits)\n",
    "    #Convert it to 0 or 1 class based on the probability and cast it to integer\n",
    "    y_pred = tf.cast(y_pred > 0.5, tf.int64)\n",
    "    #y_pred = tf.cast(tf.round(y_pred), tf.int64)\n",
    "    \n",
    "    #Create a boolean tensor by comparing the model prediction against the labels\n",
    "    correct_prediction = tf.equal(y_pred, tf.cast(Y, tf.int64))\n",
    "    #Compute the accuracy across all the instances\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q-SeFuNupU8v"
   },
   "source": [
    "### Build the computation graph & execute it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "uUyeq938pU8w"
   },
   "outputs": [],
   "source": [
    "def train_model(n_neurons, n_inputs, learning_rate = 0.01, mini_batch_size = 32, n_epochs = 20, print_cost = False):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #Declare and initialize the required variables\n",
    "    costs = []\n",
    "    \n",
    "    #Create Placeholder\n",
    "    X, Y = create_placeholders(n_inputs)\n",
    "    \n",
    "    #Create the nn_model\n",
    "    logits = nn_model(X, n_neurons) \n",
    "    \n",
    "    #Compute the cost\n",
    "    cost = compute_cost(Y, logits)\n",
    "    \n",
    "    #Optimize the cost using Gradient Descent Optimizer\n",
    "    training_op = optimizer(learning_rate, cost)\n",
    "    \n",
    "    #Evaluate the model\n",
    "    accuracy = evaluate_model(logits, Y)\n",
    "    \n",
    "    #Initialize the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    #Create the Saver object\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    #Execute the Graph - Train the model\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        \n",
    "        X_full_dataset, Y_full_dataset = load_dataset()\n",
    "        \n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            n_mini_batches = 0\n",
    "            epoch_cost = 0\n",
    "            total_time_taken_to_create_mini_batches = 0\n",
    "            \n",
    "            tic = time.time()\n",
    "            \n",
    "            shuffled_indices = get_shuffled_indices(0, 25)\n",
    "            for j in shuffled_indices:\n",
    "                X_mini = None\n",
    "                Y_mini = None\n",
    "                #X_mini, Y_mini = load_mini_dataset(j)\n",
    "                \n",
    "                X_mini = X_full_dataset[j]\n",
    "                Y_mini = Y_full_dataset[j]\n",
    "                \n",
    "                rmb_tic = time.time()\n",
    "                mini_batches = None\n",
    "                mini_batches = create_random_mini_batches(X_mini, Y_mini, mini_batch_size = mini_batch_size)\n",
    "                rmb_toc = time.time()\n",
    "                total_time_taken_to_create_mini_batches += (rmb_toc-rmb_tic)\n",
    "                            \n",
    "                for mini_batch in mini_batches:\n",
    "                    n_mini_batches += 1\n",
    "                    (X_mini_batch, Y_mini_batch) = mini_batch\n",
    "                    _, mini_batch_cost = sess.run([training_op, cost], feed_dict = {X: X_mini_batch, Y: Y_mini_batch})\n",
    "\n",
    "                    epoch_cost += mini_batch_cost\n",
    "            \n",
    "            toc = time.time()\n",
    "            training_time = ((toc-tic) - total_time_taken_to_create_mini_batches) * 1000\n",
    "            print('Epoch', epoch, \n",
    "                  '| Mini Batch Creation:', total_time_taken_to_create_mini_batches * 1000, 'ms',\n",
    "                  '| Training Time:', training_time, 'ms',\n",
    "                  '| Total Time:', ((toc-tic) * 1000), 'ms')\n",
    "            \n",
    "            epoch_cost = epoch_cost / n_mini_batches\n",
    "            train_accuracy = accuracy.eval(feed_dict = {X: X_mini_batch, Y: Y_mini_batch})\n",
    "            #cval_accuracy = accuracy.eval(feed_dict = {X: X_cval, Y: Y_cval})\n",
    "            \n",
    "            if print_cost and epoch % 2 == 0:\n",
    "                print('At epoch', epoch, 'Cost =', epoch_cost, '| Train Accuracy =', train_accuracy)\n",
    "            \n",
    "            if epoch % 2 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "            \n",
    "        saver_path = saver.save(sess, './my_model_final.ckpt')\n",
    "        \n",
    "    #print('Final - Train Accuracy =', train_accuracy, '| CVal Accuracy =', cval_accuracy)\n",
    "    print('Final - Train Accuracy =', train_accuracy)\n",
    "        \n",
    "    plt.plot(costs)\n",
    "    plt.xlabel('# of Epochs')\n",
    "    plt.ylabel('Training Error - Cost')\n",
    "    plt.title('Training Error Vs Epochs')\n",
    "    plt.show()\n",
    "    \n",
    "    return saver_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nDvKIWNHpU8z",
    "outputId": "d92a58cd-70c0-4664-ccc6-39c02bb3443b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load the data set is 52251.48677825928 ms\n",
      "Epoch 1 | Mini Batch Creation: 3055.1929473876953 ms | Training Time: 18070.298194885254 ms | Total Time: 21125.49114227295 ms\n",
      "Epoch 2 | Mini Batch Creation: 3082.13472366333 ms | Training Time: 18130.122661590576 ms | Total Time: 21212.257385253906 ms\n",
      "At epoch 2 Cost = 0.69353170994 | Train Accuracy = 0.375\n",
      "Epoch 3 | Mini Batch Creation: 3064.2781257629395 ms | Training Time: 18221.782684326172 ms | Total Time: 21286.06081008911 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-de84d0c2fa45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mn_input_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m49152\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m train_model(n_neurons, n_input_features, learning_rate = 0.01, n_epochs = 20, mini_batch_size = 32,\n\u001b[1;32m----> 9\u001b[1;33m             print_cost = True)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-12-37e37fc440b5>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(n_neurons, n_inputs, learning_rate, mini_batch_size, n_epochs, print_cost)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mn_mini_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[1;33m(\u001b[0m\u001b[0mX_mini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_mini_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m                     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmini_batch_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtraining_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mX_mini_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mY_mini_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m                     \u001b[0mepoch_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmini_batch_cost\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Softwares\\ProgramFiles\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_neurons = {'hidden_layer_1': 50,\n",
    "             'hidden_layer_2': 50,\n",
    "             'hidden_layer_3': 50,\n",
    "             'hidden_layer_4': 50,\n",
    "             'hidden_layer_5': 30,\n",
    "             'output_layer': 1}\n",
    "n_input_features = 49152\n",
    "train_model(n_neurons, n_input_features, learning_rate = 0.01, n_epochs = 20, mini_batch_size = 32,\n",
    "            print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-167b7395e63c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m#n = []\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "m, n = []\n",
    "#n = []\n",
    "m.append(0)\n",
    "m.append(1)\n",
    "n.append(2)\n",
    "n.append(3)\n",
    "print('m length =', len(m), 'm =', m[1])\n",
    "print('n =', n[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "DNN using plain TF - Cat vs Dog classifier.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
