{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DNN using plain TF - Cat vs Dog classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selvam85/Cat-Dog-Classifier/blob/master/DNN_using_plain_TF_Cat_vs_Dog_classifier_Kaggle_dataset/DNN%20using%20plain%20TF%20-%20Cat%20vs%20Dog%20classifier_Colab%20file.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "lcFhBVfsOxaJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "222bbe7a-a260-4db4-c5a7-b2c44c67238f"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8xkS3MVSpU73",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import h5py\n",
        "import math\n",
        "import time\n",
        "from random import shuffle\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.contrib.layers import fully_connected, l2_regularizer\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zWco96WUJB9Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Util functions"
      ]
    },
    {
      "metadata": {
        "id": "q2sMZB-dJB9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_mini_dataset(index):\n",
        "    \n",
        "    #filename_prefix = 'datasets/dog_vs_cat_normalized_dataset_kaggle_128px_'\n",
        "    filename_prefix = 'drive/DeepLearning/Datasets/Kaggle_Cat_Vs_Dog_normalized/dog_vs_cat_normalized_dataset_kaggle_128px_'\n",
        "    filename = filename_prefix + str(index) + \".h5\"\n",
        "    with h5py.File(filename, \"r\") as f:\n",
        "    \n",
        "        #print(list(f.keys()))\n",
        "        x_mini_data = None\n",
        "        y_mini_data = None\n",
        "        \n",
        "        x_mini_data = f[\"input_data\"][:]\n",
        "        y_mini_data = f[\"input_labels\"][:]\n",
        "    \n",
        "    return x_mini_data, y_mini_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1igTsryFJB9e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_shuffled_indices(start_value, last_value):\n",
        "    \n",
        "    temp = np.arange(start_value, last_value)\n",
        "    shuffle(temp)\n",
        "    \n",
        "    return temp"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oMRZoq2BJB9i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_random_mini_batches(X, Y, n_classes = 2, mini_batch_size = 32):\n",
        "    \n",
        "    m = X.shape[0] # number of training examples\n",
        "    mini_batches = []\n",
        "    \n",
        "    #print(\"Shape of X =\", X.shape)\n",
        "    #print(\"Shape of Y =\", Y.shape)\n",
        "    \n",
        "    #Reshaping to convert Y to a 2D array from a rank one array\n",
        "    Y = Y.reshape(Y.shape[0], 1)\n",
        "    \n",
        "    #Shuffle the data in each of the mini batch\n",
        "    permutation = list(np.random.permutation(m))\n",
        "    shuffled_X = X[permutation, :]\n",
        "    shuffled_Y = Y[permutation, :]\n",
        "    \n",
        "    n_mini_batches = math.ceil(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(n_mini_batches):\n",
        "        \n",
        "        start_pos = k * mini_batch_size\n",
        "        end_pos = min(start_pos + mini_batch_size, m)\n",
        "        \n",
        "        mini_batch_X = shuffled_X[start_pos : end_pos, :]\n",
        "        mini_batch_Y = shuffled_Y[start_pos : end_pos, :]\n",
        "        \n",
        "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
        "        mini_batches.append(mini_batch)\n",
        "    \n",
        "    return mini_batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "i56i0MpqpU8U"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the DNN using plain TensorFlow\n",
        "\n",
        "**Step 1 - Code the individual ops in the computation graph**\n",
        "- Create Placeholders for X, Y\n",
        "- Create the nn_model - all the layers and initialize them\n",
        "- Compute the cost\n",
        "- Create the optimizer to minimize the cost\n",
        "- Evaluate the model\n",
        "\n",
        "**Step 2 - Build the computation graph**\n",
        "- Combine all the steps in Step 1 to build the computation graph\n",
        "- Initialize the variables\n",
        "- Create a Saver object to save the learnt parameters after the model is trained\n",
        "\n",
        "**Step 3 - Execute the graph**\n",
        "- Create mini batches so that gradient descent works on these mini batches for every step instead of all instances\n",
        "- Train the model for a given number of epochs\n",
        "- Print the cost, train accuracy & test accuracy at regular interval of epochs\n",
        "- Plot the training error (cost) vs epochs\n",
        "- Save the parameters\n",
        "\n",
        "**Step 4 - Predict for any data**\n",
        "- Build a method to pass random data for the model to predict based on the parameters learnt through training"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Oq2WIyiCpU8V"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the placeholders"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "AkBHP2R4pU8X",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_placeholders(n_inputs):\n",
        "    '''\n",
        "    n_inputs - A scalar containing the number of input features\n",
        "    '''\n",
        "    \n",
        "    X = tf.placeholder(tf.float32, shape = (None, n_inputs), name = 'X')\n",
        "    Y = tf.placeholder(tf.float32, shape = (None, 1), name = 'Y')\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HoJ-w6NMpU8a"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the NN model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "0iig8mn_pU8c",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nn_model(X, n_neurons):\n",
        "    '''\n",
        "    X - input Tensor X\n",
        "    n_neurons - A scalar containing the number of neurons in each layer including both hidden layers and output layer \n",
        "    '''\n",
        "    #with arg_scope([fully_connected], weights_regularizer = l2_regularizer(scale = 0.01)):\n",
        "    hidden_layer_1 = fully_connected(X, n_neurons['hidden_layer_1'], scope = 'hidden_layer_1')\n",
        "    hidden_layer_2 = fully_connected(hidden_layer_1, n_neurons['hidden_layer_2'], scope = 'hidden_layer_2')\n",
        "    hidden_layer_3 = fully_connected(hidden_layer_2, n_neurons['hidden_layer_3'], scope = 'hidden_layer_3')\n",
        "    hidden_layer_4 = fully_connected(hidden_layer_3, n_neurons['hidden_layer_4'], scope = 'hidden_layer_4')\n",
        "    hidden_layer_5 = fully_connected(hidden_layer_4, n_neurons['hidden_layer_5'], scope = 'hidden_layer_5')\n",
        "    logits = fully_connected(hidden_layer_3, n_neurons['output_layer'], activation_fn = None, scope = 'output_layer')\n",
        "    \n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XZMtdnd7pU8f"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute the cost"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "iwvRAcarpU8g",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(labels, logits):\n",
        "    '''\n",
        "    labels - label tensor Y\n",
        "    logits - Tensor containing the values of the output layer before passing to the activation function\n",
        "    '''\n",
        "    \n",
        "    cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
        "    cost = tf.reduce_mean(cross_entropy, name = 'cost')\n",
        "    #base_cost = tf.reduce_mean(cross_entropy)\n",
        "    #reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
        "    #cost = tf.add_n([base_cost] + reg_losses, name = 'cost')\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "AAWyNQI6pU8m"
      },
      "cell_type": "markdown",
      "source": [
        "### Create the optimizer and the training operation"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "xk2mXeZ6pU8o",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimizer(learning_rate, cost):\n",
        "    '''\n",
        "    learning_rate - A scalar value containing the learning rate for the backpropagation step\n",
        "    cost - Overall cost from the forward propagation step for one set of mini batch instance\n",
        "    '''\n",
        "    \n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "    training_op = optimizer.minimize(cost)\n",
        "    \n",
        "    return training_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "rnft-gRwpU8r"
      },
      "cell_type": "markdown",
      "source": [
        "### Evaluate the model\n",
        "\n",
        "We will use accuracy as the evaluation metric"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "kaxqAWcmpU8r",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def evaluate_model(logits, Y):\n",
        "    '''\n",
        "    logits - An array containing the values from the output layer\n",
        "    Y - An array containing the labels\n",
        "    '''\n",
        "    \n",
        "    #Compute the probability using the sigmoid function\n",
        "    y_pred = tf.nn.sigmoid(logits)\n",
        "    #Convert it to 0 or 1 class based on the probability and cast it to integer\n",
        "    y_pred = tf.cast(y_pred > 0.5, tf.int64)\n",
        "    #y_pred = tf.cast(tf.round(y_pred), tf.int64)\n",
        "    \n",
        "    #Create a boolean tensor by comparing the model prediction against the labels\n",
        "    correct_prediction = tf.equal(y_pred, tf.cast(Y, tf.int64))\n",
        "    #Compute the accuracy across all the instances\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "q-SeFuNupU8v"
      },
      "cell_type": "markdown",
      "source": [
        "### Build the computation graph & execute it"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "uUyeq938pU8w",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_model(n_neurons, n_inputs, learning_rate = 0.01, mini_batch_size = 32, n_epochs = 50, print_cost = False):\n",
        "    \n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    #Declare and initialize the required variables\n",
        "    costs = []\n",
        "    \n",
        "    #Create Placeholder\n",
        "    X, Y = create_placeholders(n_inputs)\n",
        "    \n",
        "    #Create the nn_model\n",
        "    logits = nn_model(X, n_neurons) \n",
        "    \n",
        "    #Compute the cost\n",
        "    cost = compute_cost(Y, logits)\n",
        "    \n",
        "    #Optimize the cost using Gradient Descent Optimizer\n",
        "    training_op = optimizer(learning_rate, cost)\n",
        "    \n",
        "    #Evaluate the model\n",
        "    accuracy = evaluate_model(logits, Y)\n",
        "    \n",
        "    #Initialize the variables\n",
        "    init = tf.global_variables_initializer()\n",
        "    \n",
        "    #Create the Saver object\n",
        "    saver = tf.train.Saver()\n",
        "    \n",
        "    #Execute the Graph - Train the model\n",
        "    with tf.Session() as sess:\n",
        "        init.run()\n",
        "        \n",
        "        for epoch in range(1, n_epochs + 1):\n",
        "            n_mini_batches = 0\n",
        "            epoch_cost = 0\n",
        "            total_time_taken_to_load_dataset = 0\n",
        "            total_time_taken_to_create_mini_batches = 0\n",
        "            \n",
        "            tic = time.time()\n",
        "            \n",
        "            file_indices = get_shuffled_indices(1, 26)\n",
        "            for j in file_indices:\n",
        "                \n",
        "                X_mini = None\n",
        "                Y_mini = None\n",
        "                lmd_tic = time.time()\n",
        "                X_mini, Y_mini = load_mini_dataset(j)\n",
        "                lmd_toc = time.time()\n",
        "                total_time_taken_to_load_dataset += (lmd_toc-lmd_tic)\n",
        "                \n",
        "                rmb_tic = time.time()\n",
        "                mini_batches = None\n",
        "                mini_batches = create_random_mini_batches(X_mini, Y_mini, mini_batch_size = mini_batch_size)\n",
        "                rmb_toc = time.time()\n",
        "                total_time_taken_to_create_mini_batches += (rmb_toc-rmb_tic)\n",
        "                \n",
        "                for mini_batch in mini_batches:\n",
        "                    n_mini_batches += 1\n",
        "                    (X_mini_batch, Y_mini_batch) = mini_batch\n",
        "                    _, mini_batch_cost = sess.run([training_op, cost], feed_dict = {X: X_mini_batch, Y: Y_mini_batch})\n",
        "\n",
        "                    epoch_cost += mini_batch_cost\n",
        "            \n",
        "            toc = time.time()\n",
        "            training_time = ((toc-tic) - (total_time_taken_to_load_dataset + total_time_taken_to_create_mini_batches)) * 1000\n",
        "            print('Epoch', epoch, \n",
        "                  '| Load Dataset:', total_time_taken_to_load_dataset * 1000, 'ms', \n",
        "                  '| Mini Batch Creation:', total_time_taken_to_create_mini_batches * 1000, 'ms',\n",
        "                  '| Training Time:', training_time, 'ms',\n",
        "                  '| Total Time:', ((toc-tic)*1000), 'ms')\n",
        "            \n",
        "            epoch_cost = epoch_cost / n_mini_batches\n",
        "            train_accuracy = accuracy.eval(feed_dict = {X: X_mini_batch, Y: Y_mini_batch})\n",
        "            #cval_accuracy = accuracy.eval(feed_dict = {X: X_cval, Y: Y_cval})\n",
        "            \n",
        "            if print_cost and epoch % 2 == 0:\n",
        "                print('At epoch', epoch, 'Cost =', epoch_cost, '| Train Accuracy =', train_accuracy)\n",
        "            \n",
        "            if epoch % 2 == 0:\n",
        "                costs.append(epoch_cost)\n",
        "            \n",
        "        saver_path = saver.save(sess, './my_model_final.ckpt')\n",
        "        \n",
        "    #print('Final - Train Accuracy =', train_accuracy, '| CVal Accuracy =', cval_accuracy)\n",
        "    print('Final - Train Accuracy =', train_accuracy)\n",
        "        \n",
        "    plt.plot(costs)\n",
        "    plt.xlabel('# of Epochs')\n",
        "    plt.ylabel('Training Error - Cost')\n",
        "    plt.title('Training Error Vs Epochs')\n",
        "    plt.show()\n",
        "    \n",
        "    return saver_path"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "nDvKIWNHpU8z",
        "outputId": "606cf71b-490c-4971-f966-3d16fc1ba66b",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2247
        }
      },
      "cell_type": "code",
      "source": [
        "n_neurons = {'hidden_layer_1': 50,\n",
        "             'hidden_layer_2': 50,\n",
        "             'hidden_layer_3': 50,\n",
        "             'hidden_layer_4': 50,\n",
        "             'hidden_layer_5': 30,\n",
        "             'output_layer': 1}\n",
        "n_input_features = 49152\n",
        "train_model(n_neurons, n_input_features, learning_rate = 0.01, n_epochs = 200, mini_batch_size = 32,\n",
        "            print_cost = True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 | Load Dataset: 77741.11914634705 ms | Mini Batch Creation: 2099.8308658599854 ms | Training Time: 5125.489711761475 ms | Total Time: 84966.4397239685 ms\n",
            "Epoch 2 | Load Dataset: 72758.376121521 ms | Mini Batch Creation: 2012.336254119873 ms | Training Time: 4368.588209152222 ms | Total Time: 79139.30058479309 ms\n",
            "At epoch 2 Cost = 0.6933276585453306 | Train Accuracy = 0.53125\n",
            "Epoch 3 | Load Dataset: 67523.70762825012 ms | Mini Batch Creation: 1907.8521728515625 ms | Training Time: 4424.175500869751 ms | Total Time: 73855.73530197144 ms\n",
            "Epoch 4 | Load Dataset: 66563.90047073364 ms | Mini Batch Creation: 1974.287748336792 ms | Training Time: 4389.898061752319 ms | Total Time: 72928.08628082275 ms\n",
            "At epoch 4 Cost = 0.6932923645924425 | Train Accuracy = 0.65625\n",
            "Epoch 5 | Load Dataset: 67743.43013763428 ms | Mini Batch Creation: 1868.1931495666504 ms | Training Time: 4439.467906951904 ms | Total Time: 74051.09119415283 ms\n",
            "Epoch 6 | Load Dataset: 66158.49208831787 ms | Mini Batch Creation: 1935.9564781188965 ms | Training Time: 4465.441942214966 ms | Total Time: 72559.89050865173 ms\n",
            "At epoch 6 Cost = 0.6933910552498019 | Train Accuracy = 0.625\n",
            "Epoch 7 | Load Dataset: 68763.76152038574 ms | Mini Batch Creation: 1835.9160423278809 ms | Training Time: 4367.1863079071045 ms | Total Time: 74966.86387062073 ms\n",
            "Epoch 8 | Load Dataset: 67557.39068984985 ms | Mini Batch Creation: 1916.9507026672363 ms | Training Time: 4423.569202423096 ms | Total Time: 73897.91059494019 ms\n",
            "At epoch 8 Cost = 0.6933947315301432 | Train Accuracy = 0.375\n",
            "Epoch 9 | Load Dataset: 66599.53117370605 ms | Mini Batch Creation: 1901.237964630127 ms | Training Time: 4465.555191040039 ms | Total Time: 72966.32432937622 ms\n",
            "Epoch 10 | Load Dataset: 67574.42498207092 ms | Mini Batch Creation: 1886.0588073730469 ms | Training Time: 4458.1968784332275 ms | Total Time: 73918.6806678772 ms\n",
            "At epoch 10 Cost = 0.6933601672387184 | Train Accuracy = 0.5625\n",
            "Epoch 11 | Load Dataset: 80076.78747177124 ms | Mini Batch Creation: 1905.5206775665283 ms | Training Time: 4465.954542160034 ms | Total Time: 86448.2626914978 ms\n",
            "Epoch 12 | Load Dataset: 66720.22271156311 ms | Mini Batch Creation: 1894.6692943572998 ms | Training Time: 4482.515096664429 ms | Total Time: 73097.40710258484 ms\n",
            "At epoch 12 Cost = 0.6933238420949872 | Train Accuracy = 0.46875\n",
            "Epoch 13 | Load Dataset: 66524.91497993469 ms | Mini Batch Creation: 1934.7491264343262 ms | Training Time: 4414.140939712524 ms | Total Time: 72873.80504608154 ms\n",
            "Epoch 14 | Load Dataset: 64134.8443031311 ms | Mini Batch Creation: 1899.0483283996582 ms | Training Time: 4357.1271896362305 ms | Total Time: 70391.01982116699 ms\n",
            "At epoch 14 Cost = 0.6933146077196312 | Train Accuracy = 0.4375\n",
            "Epoch 15 | Load Dataset: 67546.70453071594 ms | Mini Batch Creation: 1922.1858978271484 ms | Training Time: 4467.6549434661865 ms | Total Time: 73936.54537200928 ms\n",
            "Epoch 16 | Load Dataset: 66607.38062858582 ms | Mini Batch Creation: 1852.8449535369873 ms | Training Time: 4347.750186920166 ms | Total Time: 72807.97576904297 ms\n",
            "At epoch 16 Cost = 0.6934226038663284 | Train Accuracy = 0.53125\n",
            "Epoch 17 | Load Dataset: 65637.79640197754 ms | Mini Batch Creation: 1889.0821933746338 ms | Training Time: 4351.3853549957275 ms | Total Time: 71878.2639503479 ms\n",
            "Epoch 18 | Load Dataset: 64146.08550071716 ms | Mini Batch Creation: 1876.2810230255127 ms | Training Time: 4407.18674659729 ms | Total Time: 70429.55327033997 ms\n",
            "At epoch 18 Cost = 0.6934385115990553 | Train Accuracy = 0.5625\n",
            "Epoch 19 | Load Dataset: 68322.42488861084 ms | Mini Batch Creation: 1849.7917652130127 ms | Training Time: 4416.404485702515 ms | Total Time: 74588.62113952637 ms\n",
            "Epoch 20 | Load Dataset: 67694.3211555481 ms | Mini Batch Creation: 1894.5159912109375 ms | Training Time: 4312.283515930176 ms | Total Time: 73901.12066268921 ms\n",
            "At epoch 20 Cost = 0.6934059944451617 | Train Accuracy = 0.5\n",
            "Epoch 21 | Load Dataset: 65686.68460845947 ms | Mini Batch Creation: 1877.7618408203125 ms | Training Time: 4426.74994468689 ms | Total Time: 71991.19639396667 ms\n",
            "Epoch 22 | Load Dataset: 67096.1365699768 ms | Mini Batch Creation: 1917.6902770996094 ms | Training Time: 4408.1103801727295 ms | Total Time: 73421.93722724915 ms\n",
            "At epoch 22 Cost = 0.693491624444342 | Train Accuracy = 0.625\n",
            "Epoch 23 | Load Dataset: 67558.15768241882 ms | Mini Batch Creation: 1871.1216449737549 ms | Training Time: 4434.2639446258545 ms | Total Time: 73863.54327201843 ms\n",
            "Epoch 24 | Load Dataset: 67482.66839981079 ms | Mini Batch Creation: 1880.8956146240234 ms | Training Time: 4389.375448226929 ms | Total Time: 73752.93946266174 ms\n",
            "At epoch 24 Cost = 0.6933443068390917 | Train Accuracy = 0.625\n",
            "Epoch 25 | Load Dataset: 66732.57946968079 ms | Mini Batch Creation: 1888.6091709136963 ms | Training Time: 4404.759883880615 ms | Total Time: 73025.9485244751 ms\n",
            "Epoch 26 | Load Dataset: 69928.46322059631 ms | Mini Batch Creation: 1841.928243637085 ms | Training Time: 4414.910793304443 ms | Total Time: 76185.30225753784 ms\n",
            "At epoch 26 Cost = 0.6934731570656038 | Train Accuracy = 0.4375\n",
            "Epoch 27 | Load Dataset: 67137.48598098755 ms | Mini Batch Creation: 1892.6239013671875 ms | Training Time: 4392.134428024292 ms | Total Time: 73422.24431037903 ms\n",
            "Epoch 28 | Load Dataset: 66672.33109474182 ms | Mini Batch Creation: 1841.707706451416 ms | Training Time: 4423.2189655303955 ms | Total Time: 72937.25776672363 ms\n",
            "At epoch 28 Cost = 0.6932648191671542 | Train Accuracy = 0.46875\n",
            "Epoch 29 | Load Dataset: 67857.78450965881 ms | Mini Batch Creation: 1921.6952323913574 ms | Training Time: 4490.9608364105225 ms | Total Time: 74270.4405784607 ms\n",
            "Epoch 30 | Load Dataset: 67200.10256767273 ms | Mini Batch Creation: 1869.3342208862305 ms | Training Time: 4407.679319381714 ms | Total Time: 73477.11610794067 ms\n",
            "At epoch 30 Cost = 0.6934440517059678 | Train Accuracy = 0.53125\n",
            "Epoch 31 | Load Dataset: 67172.60193824768 ms | Mini Batch Creation: 1892.8477764129639 ms | Training Time: 4433.125019073486 ms | Total Time: 73498.57473373413 ms\n",
            "Epoch 32 | Load Dataset: 66440.27876853943 ms | Mini Batch Creation: 1842.5195217132568 ms | Training Time: 4396.80027961731 ms | Total Time: 72679.59856987 ms\n",
            "At epoch 32 Cost = 0.6934474458932267 | Train Accuracy = 0.28125\n",
            "Epoch 33 | Load Dataset: 66597.4931716919 ms | Mini Batch Creation: 1891.4034366607666 ms | Training Time: 4439.3370151519775 ms | Total Time: 72928.23362350464 ms\n",
            "Epoch 34 | Load Dataset: 67851.57632827759 ms | Mini Batch Creation: 1907.7816009521484 ms | Training Time: 4377.634286880493 ms | Total Time: 74136.99221611023 ms\n",
            "At epoch 34 Cost = 0.6933795490380749 | Train Accuracy = 0.625\n",
            "Epoch 35 | Load Dataset: 66418.71452331543 ms | Mini Batch Creation: 1903.8829803466797 ms | Training Time: 4452.866315841675 ms | Total Time: 72775.46381950378 ms\n",
            "Epoch 36 | Load Dataset: 67371.77968025208 ms | Mini Batch Creation: 1853.17063331604 ms | Training Time: 4377.393484115601 ms | Total Time: 73602.34379768372 ms\n",
            "At epoch 36 Cost = 0.6934652788102474 | Train Accuracy = 0.5625\n",
            "Epoch 37 | Load Dataset: 71595.82376480103 ms | Mini Batch Creation: 1904.5844078063965 ms | Training Time: 4432.0478439331055 ms | Total Time: 77932.45601654053 ms\n",
            "Epoch 38 | Load Dataset: 66615.91863632202 ms | Mini Batch Creation: 1856.3549518585205 ms | Training Time: 4383.297920227051 ms | Total Time: 72855.5715084076 ms\n",
            "At epoch 38 Cost = 0.6933175104353434 | Train Accuracy = 0.46875\n",
            "Epoch 39 | Load Dataset: 67343.6667919159 ms | Mini Batch Creation: 1889.2302513122559 ms | Training Time: 4432.812213897705 ms | Total Time: 73665.70925712585 ms\n",
            "Epoch 40 | Load Dataset: 68334.44356918335 ms | Mini Batch Creation: 1800.0001907348633 ms | Training Time: 4398.727893829346 ms | Total Time: 74533.17165374756 ms\n",
            "At epoch 40 Cost = 0.6933326721191406 | Train Accuracy = 0.46875\n",
            "Epoch 41 | Load Dataset: 67740.61965942383 ms | Mini Batch Creation: 1843.247413635254 ms | Training Time: 4412.895441055298 ms | Total Time: 73996.76251411438 ms\n",
            "Epoch 42 | Load Dataset: 68608.66069793701 ms | Mini Batch Creation: 1877.6757717132568 ms | Training Time: 4311.986207962036 ms | Total Time: 74798.3226776123 ms\n",
            "At epoch 42 Cost = 0.6933813367200934 | Train Accuracy = 0.375\n",
            "Epoch 43 | Load Dataset: 67162.49465942383 ms | Mini Batch Creation: 1882.2379112243652 ms | Training Time: 4387.857675552368 ms | Total Time: 73432.59024620056 ms\n",
            "Epoch 44 | Load Dataset: 69760.35237312317 ms | Mini Batch Creation: 1833.1964015960693 ms | Training Time: 4428.89142036438 ms | Total Time: 76022.44019508362 ms\n",
            "At epoch 44 Cost = 0.6933425453007983 | Train Accuracy = 0.3125\n",
            "Epoch 45 | Load Dataset: 65820.36328315735 ms | Mini Batch Creation: 1860.4633808135986 ms | Training Time: 4398.264408111572 ms | Total Time: 72079.09107208252 ms\n",
            "Epoch 46 | Load Dataset: 66569.42749023438 ms | Mini Batch Creation: 1872.101068496704 ms | Training Time: 4380.911111831665 ms | Total Time: 72822.43967056274 ms\n",
            "At epoch 46 Cost = 0.6932927052992994 | Train Accuracy = 0.71875\n",
            "Epoch 47 | Load Dataset: 69236.59420013428 ms | Mini Batch Creation: 1847.325325012207 ms | Training Time: 4405.500173568726 ms | Total Time: 75489.41969871521 ms\n",
            "Epoch 48 | Load Dataset: 66975.71086883545 ms | Mini Batch Creation: 1867.624282836914 ms | Training Time: 4359.452247619629 ms | Total Time: 73202.78739929199 ms\n",
            "At epoch 48 Cost = 0.6934161466710708 | Train Accuracy = 0.46875\n",
            "Epoch 49 | Load Dataset: 66911.4556312561 ms | Mini Batch Creation: 1820.3520774841309 ms | Training Time: 4396.589040756226 ms | Total Time: 73128.39674949646 ms\n",
            "Epoch 50 | Load Dataset: 65889.87302780151 ms | Mini Batch Creation: 1873.6600875854492 ms | Training Time: 4333.5862159729 ms | Total Time: 72097.11933135986 ms\n",
            "At epoch 50 Cost = 0.6933915465689071 | Train Accuracy = 0.46875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e45dc27316c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mn_input_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m49152\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m train_model(n_neurons, n_input_features, learning_rate = 0.01, n_epochs = 200, mini_batch_size = 32,\n\u001b[0;32m----> 9\u001b[0;31m             print_cost = True)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-88e27d14c9de>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(n_neurons, n_inputs, learning_rate, mini_batch_size, n_epochs, print_cost)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mY_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mlmd_tic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mX_mini\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_mini\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mini_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mlmd_toc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtotal_time_taken_to_load_dataset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlmd_toc\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlmd_tic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-6ca7c09deeb0>\u001b[0m in \u001b[0;36mload_mini_dataset\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mfilename_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/DeepLearning/Datasets/Kaggle_Cat_Vs_Dog_normalized/dog_vs_cat_normalized_dataset_kaggle_128px_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m#print(list(f.keys()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, **kwds)\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muserblock_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswmr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswmr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}